# Spark MLlib学习笔记

Spark的MLlib库基本上已经过时，现在主流的是ml库，所以玩玩就好。

一些算法可以参考github项目 https://github.com/scutan90/DeepLearning-500-questions

- spark官方推荐使用ml, 因为ml功能更全面更灵活，未来会主要支持ml，mllib很有可能会被废弃(据说可能是在spark3.0中deprecated）。
- ml中无论是什么模型，都提供了统一的算法操作接口，比如模型训练都是`fit`；不像mllib中不同模型会有各种各样的`trainXXX`。
- mllib在spark2.0之后进入`维护状态`, 这个状态通常只修复BUG不增加新功能。

## reduceByKey和groupByKey区别

先上结论

> 在对大数据进行复杂计算时，reduceByKey优于groupByKey。
>
> 另外，如果仅仅是group处理，那么以下函数应该优先于 groupByKey ：
> 　　（1）、combineByKey 组合数据，但是组合之后的数据类型与输入时值的类型不一样。
> 　　（2）、foldByKey合并每一个 key 的所有值，在级联函数和“零值”中使用。





reduceByKey用于对每个key对应的多个value进行merge操作，最重要的是它能够在本地先进行merge操作，并且merge操作可以通过函数自定义。

groupByKey也是对每个key进行操作，但只生成一个sequence。需要特别注意“Note”中的话，它告诉我们：如果需要对sequence进行aggregation操作（注意，groupByKey本身不能自定义操作函数），那么，选择reduceByKey/aggregateByKey更好。这是因为groupByKey不能自定义函数，我们需要先用groupByKey生成RDD，然后才能对此RDD通过map进行自定义函数操作。



用法（结果一样，内部处理方式不同）

~~~scala
val words = Array("one", "two", "two", "three", "three", "three")  
  
val wordPairsRDD = sc.parallelize(words).map(word => (word, 1))  

val wordCountsWithReduce = wordPairsRDD.reduceByKey(_ + _)  
val wordCountsWithGroup = wordPairsRDD.groupByKey().map(t => (t._1, t._2.sum))  
~~~



当采用reduceByKeyt时，Spark可以在每个分区移动数据之前将待输出数据与一个共用的key结合。借助下图可以理解在reduceByKey里究竟发生了什么。 注意在数据对被搬移前同一机器上同样的key是怎样被组合的(reduceByKey中的lamdba函数)。然后lamdba函数在每个区上被再次调用来将所有值reduce成一个最终结果。整个过程如下：



![image-20201007171314279](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201007171314279.png)



当采用groupByKey时，由于它不接收函数，spark只能先将所有的键值对(key-value pair)都移动，这样的后果是集群节点之间的开销很大，导致传输延时。整个过程如下：

![image-20201007171355789](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201007171355789.png)







## RDD

RDD可以看成是一个简单的“数组”，对其进行操作只需要调用有限的数组中的方法即可。与一般数组的区别在于：RDD是分布式存储，可以更好地利用现有的云数据平台，并在内存中运行（省去大量与硬盘的IO）。本质是存储在不同节点计算机中的数据集



在Spark中，弹性指的是数据的存储方式，即数据在节点中进行存储的时候，既可以使用内存也可以使用硬盘；弹性还有一层意思是，RDD具有很强的容错性，具体指Spark在运行计算的过程中，不会因为某个节点错误而使得整个任务失败。不同节点并发运行的数据，如果在某一个节点发生错误时，RDD会自动将其在不同的节点中重试。



### RDD工作原理

RDD可以将其看成一个分布在不同节点中的分布式数据集，并将数据以数据块（Block）的形式存储在各个节点的计算机中，如下图所示



![image-20201007175229211](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201007175229211.png)



从上图可以看到，每个Block Master 管理着若干个BlockSlave，而每个BlockSlaver又管理着若干个BlockNode。当BlockSlave获得了每个Node节点的地址，又会反向BlockMaster注册每个Node的基本信息，形成分层管理。

而对于某个节点中存储的数据，如果使用频率较多，则BlockMaster会将其缓存在自己的内存中，这样如果以后需要调用这些数据，则可以直接从BlockMaster中读取。对于不再使用的数据，BlockMaster会向BlockSlave发送一组命令予以销毁。



> tip
>
> 窄依赖便于在单一节点上按次序执行任务，使任务可控。宽依赖更多的是考虑任务的交互和容错性。





## chapter04 MLlib 基本概念

RDD将存储数据转化成向量和矩阵的形式进行存储和计算，这样将数据定量化表示，能更准确地整理和分析结果



### MLlib 基本数据类型

MLlib支持较多的数据格式，从最基本的Spark数据集RDD到部署在集群中的向量和矩阵

MLlib目前只支持整数与浮点数。其他类型均不支持，这与MLlib用途有关，主要用于数值计算



MLlib基本数据类型

| 类型名称           | 释义                                                 |
| ------------------ | ---------------------------------------------------- |
| Local vector       | 本地向量集。主要向spark提供一组可进行操作的数据集和  |
| Labeled point      | 向量标签。让用户能够分类不同的数据集和               |
| Local matrix       | 本地矩阵。将数据集合以矩阵形式存储在本地计算机中     |
| Distributed matrix | 分布式矩阵。将数据集合以矩阵形式存储在分布式计算机中 |



### 两组数据相关系数计算

相关系数是一种用来反映变量之间相关关系密切程度的统计指标，在现实中一般用于对两组数据的`拟合`和`相似程度`进行定量化分析。常用的一般是皮尔逊相关系数，MLlib中默认的相关系数求法也是使用皮尔逊相关系数发。斯皮尔曼相关系数用的比较少，但是其能够比较好地反映不同数据集的趋势程度。



皮尔逊相关系数计算公式
$$
\rho_{xy} = 
\frac{
	\sum(x-\overline{x})(y-\overline{y})
}{
	\sqrt{\sum(x-\overline{x}){2}  \sum(y-\overline{y}){2}}
}
$$
可以看做是两组数据的向量夹角的余弦，用来描述两组数据的分开程度。



斯皮尔曼相关系数计算公式
$$
\rho_{xy} = 1-
\frac{
	6\sum(x_i-y_i){2}
}{
	n(n^{2}-1)
}
$$

> n为数据个数



总结：

皮尔逊相关系数代表两组数据的余弦分开程度，表示随着数据量的增加，两组数据差别将增大

斯皮尔曼相关系数更注重两组数据的拟合程度，即两组数据随着数据量的增加而增长曲线不变



## chapter05 协同过滤算法



协同过滤算法是最常用的推荐算法，主要有两种具体形式：基于用户的推荐算法 和 基于物品的推荐算法。  推荐算法的基础是基于两个对象之间的相关性。



### 协同过滤

协同过滤算法是一种基于群体用户或者物品的典型推荐算法，是目前常用推荐算法中最常用和最经典的算法。

协同过滤算法主要有两种

- 一是通过考察具有相同爱好的用户对相同物品的评分标准进行计算
- 而是考察具有相同特质的物品从而推荐给选择了某件物品的用户

总体来说，协同过滤算法是建立在基于某种物品和用户之间相互关联的数据关系之上。



#### 基于用户的推荐

一句话概括：给推荐`志趣相投`的，俩人有两个物品都喜欢，则第一个人喜欢的另外一个物品，第二个人可能也喜欢

#### 基于物品的推荐

给推荐`物以类聚`的，以已有物品为线索去进行相似度计算从而推荐给特定的目标用户。



#### 协同过滤算法的不足

> 基于用户的推荐算法，针对某些热点物品的处理不够准确，对于一些常用的物品推荐，其计算结果往往排在推荐的首位，而这样的推荐没有实际应用意义。同时基于用户的推荐算法，往往数据量较为庞大，计算费事，由于热点的存在准确度也很成问题
>
> 基于物品的推荐算法相对于基于用户的推荐算法，其数据量小很多，可以较为容易地生成推荐值，但是其存在推荐同样（同类型）物品的问题。例如，用户购买了某件商品，那么推荐系统可能会继续推荐相同类型的商品给用户，用户在购买一件商品后，绝对不会再购买同类型的商品，这样的推荐是失败的。

### 相似度度量

协同过滤的最重要部分是相似度求得



#### 欧几里得距离

三维空间两个点的真实距离。

由于在欧几里得距离相似度计算中，最终数值的大小与相似度成反比，因此在实际应用场景中常常使用欧几里得距离的倒数作为相似度值，即 d/d+1 作为近似值

> 就是计算勾股定理的长边

#### 余弦相似度

与欧几里得距离相似，余弦相似度也将特定目标，即物品或者用户作为坐标上的点，但不是坐标原点。基于此与特定的被计算目标进行夹角计算。

> 就是计算夹角



#### 欧几里得相似度和余弦相似度比较

欧几里得相似度是以目标绝对距离为衡量的标准，而余弦相似度是以目标差异的大小作为衡量标准。

![image-20201008161851096](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201008161851096.png)

从上图可以看出，欧几里得相似度注重目标之间的差异，与目标在空间中的位置直接相关。而余弦相似度是不同目标在空间中的夹角，更加表现在前进趋势上的差异。

一般来说欧几里得用来表现不同目标的绝对差异性，分析目标之间的相似度与差异情况。余弦相似度更多的是对目标从方向趋势上区分，对特定坐标数字不敏感。



### ALS算法 - 交替最小二乘法

交替最小二乘法是统计分析中最常用的逼近计算中的一种算法，其交替计算结果使得最终结果尽可能地逼近真实结果。

#### 最小二乘法（LS算法）

LS算法是ALS算法的基础，LS算法是一种数学优化技术。通过最小化误差的平方和寻找数据的最佳函数配比。利用最小二乘法可以简便地求得未知数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。最小二乘法还可用于曲线拟合。其他一些优化问题也可通过最小化能量或最大化熵用最小二乘法来表达。

LS算法原理

![image-20201008164026358](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201008164026358.png)

若干个点分布在向量空间中，如果希望找出一条直线和这些点达到最佳匹配。最简单的一个方法就是希望这些点到直线的距离值最小。

f(x) 是直接拟合公式，即目标函数
$$
f(x)=ax+b
$$

$$
\delta=\sum(f(x_i)-y_i)^2
$$





#### 交替最小二乘法（ALS算法）

比较难，详细理解还需要再百度

假设基于用户名、物品表的用户评分矩阵可以被分解成2个较为小型化的矩阵，即矩阵 U 和矩阵 V 。因此可以将原始矩阵近似表示为：W=U*V

这里U和V分别表示用户和物品的矩阵，在MLlib的ALS算法中，首先对U或者V矩阵随机化生成，之后固定某一个特定对象，去求另一个未随机化的矩阵对象。然后利用被求取的矩阵对象去求随机化矩阵对象。最后两个对象相互迭代计算，求取与实际矩阵差异达到程序设定的最下阈值位置。如下所示

![image-20201008171214175](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201008171214175.png)



## chapter06 线性回归

> 解决了对某个具体数据进行预测的问题。

回归分析是一种用来确定两种或两种以上变量间相互依赖的定量关系的统计分析方法。分析可以按以下要素分类

- 按照涉及的自变量的多少，分为回归和多重回归分析
- 按照自变量的多少，可分为一元回归分析和多元回归分析
- 按照自变量与因变量之间的关系类型，可分为线性回归分析和非线性回归分析

> 在回归分析中，只包括一个自变量和一个因变量，且二者关系可用一条直线近似表示，称为一元线性回归分析。
>
> 如果回归分析中包括两个或两个以上的自变量，且因变量与自变量之间是线性关系，则称为多重线性回归分析。



### 随机梯度下降算法

机器学习中回归算法有 神经网络回归算法、蚁群回归算法、支持向量机回归算法等。MLlib中使用的是较为经典的随机梯度下降算法，它充分利用Spark框架的迭代计算特性，通过不停地判断和选择当前目标下的最优路径，从而能够在最短路径下达到最优的结果，继而提过大数据计算效率



道士下山

![image-20201008173958281](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201008173958281.png)

> 简单来说，梯度下降就是从山顶找一条最短的路走到山脚最低的地方。但是因为选择方向的原因，我们找到的的最低点可能不是真正的最低点。如图所示，黑线标注的路线所指的方向并不是真正的地方。
>
> 总结起来就一句话：随机选择一个方向，然后每次迈步都选择最陡的方向，直到这个方向上能达到的最低点。



梯度下降算法模型

![image-20201008174753527](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201008174753527.png)



### 回归的过拟合

详细理解需要百度

消除回归算法中的误差，通过增加系数等方法修正过拟合。

消除过拟合的过程称为回归的正则化。正则化使用较多的一般有两种方法，lasso回归（L1回归）和岭回归（L2回归），其目的是通过对最小二乘法估计加入惩罚因素，使某些系数的估计为0。



### 线性回归

MLlib中，线性回归的基本数据是严格按照数据格式进行设置。例如，如果想求得公式
$$
y=2x_1+3x_2
$$
的系数，那么需要在数据基础中设置2个x值，并且在其前部设置y值

~~~markdown
1,0 1
2,0 2
3,0 3
5,1 4
7,6 1
9,4 5
6,3 3

逗号前面的数值是根据不同的数据求出的结果值，而每个系数x值依次地被排列在其后，这些就是数据收集的规则：Y=a+bX

~~~



### 回归曲线的验证

使用均方误差 MSE

## chapter07 分类

分类算法又称分类器，是数据挖掘和机器学习领域中的一个非常重要的分支和方向。

### 逻辑回归

逻辑回归不是回归算法，是`分类算法`



- 逻辑回归和线性回归类似，但它不属于回归分析家族，差异主要在于变量不同，因此其解法和生成的曲线也不尽相同。
- 逻辑回归一般用于`对某些数据或事物的归属及可能性进行评估`。比如，探索某疾病的危险因素，根据危险因素预测某疾病发生的概率。

例如：

> 想探讨胃癌发生的危险因素，可以选择两组人群，一组是胃癌组，一组是非胃癌组，两组人群肯定有不同的体征和生活方式等。这里的因变量就是是否胃癌，即"是"或"否"，为两分类变量，自变量既可以包括很多了，例如年龄、性别、饮食习惯、幽门螺杆菌感染等。自变量可以是连续的，也可以是分类的。
>
> 在上面胃癌的例子中，尽管收集了各种变量因素，但是在胃癌被确诊前，任何人都无法对某人是否将来会诊断出胃癌做出断言，而只能说“有可能”患有胃癌。这个就是逻辑回归。他不直接告诉你结果的具体数据而会告诉你可能性在哪里。



~~~markdown
1|2
1|3
1|4
1|5
1|6
0|7
0|8
0|9
0|10
0|11
~~~

分隔符用以标示分类结果和数据组。用y和x表示，则y为 0或1，x为数据集中数据的特征向量。

逻辑回归的公式
$$
f(x)=
\frac{
	1
}{
	1+exp(-\theta^Tx)
}
$$
与线性回归相同， 这里的$$\theta$$ 是逻辑回归的参数，即逻辑系数，如果再将其进一步变形，使其能够反映二元分类问题的公式为：
$$
f(y=1|x,\theta)=
\frac{
	1
}{
	1+exp(-\theta^Tx)
}
$$
这里y值是由已有的数据集中数据和$$\theta$$共同决定。实际上这个公式求计算是在满足一定条件下，最终取值的对数机滤，即由数据集的可能性的比值的对数变换得到。通过公式表现为：
$$
log(x)=
ln(
\frac{
	f(y=1|x,\theta)
}{
	f(y=0|x,\theta)
}
)=
\theta_0+\theta_1x_1+\theta_2x_2+...++\theta_nx_n
$$
通过逻辑回归的倒推公式可看出，最终逻辑回归的计算可以转化成数据集的特征向量与系数$$\theta$$共同完成，然后求得其`加权和`作为最终的判断结果。



### 支持向量机

支持向量机是数据挖掘中的一个新方法，可以非常成功地处理回归（时间序列分析）和模式识别（分类问题，判别分析）等诸多问题，并可推广到预测和综合评价等领域。

MLlib对支持向量机算法有较好的支持，可以用来解决一般线性回归和逻辑回归不好处理的数据分类内容，结果验证其准确性较好。



<img src="https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201011153954199.png" alt="image-20201011153954199" style="zoom:67%;" />



两条虚线被成为支持向量，通过支持向量从而获得分类平面的方法，称为支持向量机。这个是二维平面的，有高维空间的支持向量机，具体学习要百度。



### 朴素贝叶斯



朴素贝叶斯是基于假设的先验概率、给定假设下观察到不同数据的概率，以及观察到的数据本身而得出的。其方法为，将关于未知的参数的先验信息与样本信息综合，再根据贝叶斯公式，得出后验信息，然后根据后验信息去推断未知参数的方法。



https://zh.wikipedia.org/wiki/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8

应用场景

- 通过一些测量的特征，包括身高、体重、脚的尺寸，判定一个人是男性还是女性。
- 考虑一个基于内容的文本分类问题，例如判断邮件是否为垃圾邮件。

具体学习看文本开始的github项目



## chapter08 决策树与保序回归

决策树是分类算法的一个分支，决策树是一种监管学习，所谓监管学习，就是给定一堆样本，每个样本都有一组属性和一个类别，这些类别是事先确定的，那么通过学习得到一个分类器，这个分类器能够对新出现的对象给出正确的分类。目前决策树是分类算法中应用较多的算法之一，其原理是从一组无序无规律的因素中归纳总结出符合要求的分类规则。

随机雨林顾名思义，是决策树的一种大规模应用形式，其充分利用了大规模计算机并发计算的优势，可以对数据进行并行的处理和归纳分类。

任何一个只要符合`k-v`模式的分类数据都可以根据决策树进行推断。



### 决策树

MLlib中决策树是一种典型的回归算法。与线性回归和逻辑回归不同，它在处理数据缺失和非线性方面有较多的应用价值。

决策树是在已知各种情况发生概率的基础上，通过构成决策树来求取净现值的期望值大于等于0的概率，评价项目风险，判断其可行性的决策分析方法，是直观运用概率分析的一种图解法。



![image-20201011165019100](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201011165019100.png)

> 每个分支和树叶代表一个分支向量，每个节点代表一个输出结果或分类
>
> 决策树用来预测的对象是固定的，从根到叶子节点的一条特定路线就是一个分类规则，决定这一个分类算法和结果。
>
> 决策树的生成算法是从根部开始，输入一系列带有标签分类的示例（向量），从而构造出一些列决策节点。这些节点又称为逻辑判断，表示该属性的某个分支（属性），供下一步继续判定，一般有几个分支就有几条有向有向的线作为类别标记



#### 信息熵

信息熵是决策树的理论基础

信息熵，指的是对事件中不确定的信息的度量。一个事件或者属性中，其信息熵越大，其含有的不确定信息越大，对数据分析的计算也越有益。因此，信息熵的选择总是选择当前事件中拥有最高信息熵的那个属性作为待测属性。



#### ID3算法

ID3算法是基于信息熵的一种经典决策树构建算法，ID3是一种贪心算法。以信息熵的下降速度为选取测试属性的标准，即在每个节点选取还尚未被用来划分的、具有最高信息增益的属性作为划分标准，然后继续这个过程，直到生成的决策树能完美分类训练样例。

可以说，ID3算法核心就是信息增益的计算。



#### 随机雨林与梯度提升算法

随机雨林与梯度提升是==利用分布式并发处理系统构建的并发式决策树==。

![image-20201011180007947](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201011180007947.png)





> 雨林是若干个树的集合。随机雨林从名称上看，就是若干个决策树所组成的一个决策树林。在MLlib中，随机雨林中的每一棵树都被分配到不同的节点上进行并行计算，或者在一些特定的条件下，单独的每一
> 棵决策树都可以并行化计算，每一棵决策树之间是没有相关性的。
>
> 当随机雨林在运行的时候，每当一个新的数据被传输到系统中，则会由随机雨林中的每一棵决策树同时进行处理。如果结果是一个连续常数，则对每一棵的结果取得平均值作为结果。如果是非连续结果，则选
> 择所有决策树计算最多的一项作为数据计算的结果。



**梯度提升算法**

梯度提升算法的思想类似于前面学习线性回归时的随机梯度下降算法。一个模型中有若干个属性值构成，每个属性值在开始训练时具有相同的权重，之后不断地将模型计算结果与真实值进行比较。如果出错则
降低在特定方向的损失。这段话比较拗口，由于梯度提升算法较为复杂，这里笔者只简单地提及其基本原理，其数学基础和公式请感兴趣的读者参考相关资料学习。



### 保序回归

相对于决策树，保序回归的应用范围没有决策树算法那么广泛，但是在一般应用时，特别是数据处理较为庞大的情况下，采用保序回归做回归分析可以极大地节省资源，从而提高计算效率。



保序回归是数理统计中的一种回归计算方法，它在有约束的条件下，对数据进行回归处理。其处理方法是对数据列的均值进行处理从而获得一个回归序列。



> 我们用一个简单的例子描述保序回归。例如，给定一个无序数据集，要求预测数据集中某个位置数值的大小。但是由于系统内容空间有限，数据集过大，因此有个约束条件，就是在计算时不能够对数据进行排序或者无法进行排序，但是可以对数据进行修改。
>
> 保序回归的思想是对数据进行均值排序，从数据集的第一个数开始，如果下一个数出现乱序，即与设定的顺序不符，则从乱序的数据开始逐个开始求得平均值，直到求得的平均值与下一个数据比较不成为乱序为止。例如一个数据集：
> `{ 1 , 3 , 2 , 4 , 5 }`
>
> 要求将其按保序回归由小到大进行排列。首先观察第一个数是1，可以不做变动继续存放。第二个是3，仍旧不需要变动。第三个数是2，则属于乱序从而需要对其重新计算。
>
> 第三个数是乱序，需要从其开始计算，提取数据2和下一个数据4，计算得到其平均值为3，因此可获得一个新的数据集：
> `{ 1 , 3 , 3 , 3 , 5 }`
> 继续观察下一个数值，符合排列要求到达最后一个值，从而完成保序回归。



## chapter09 聚类

聚类是一种数据挖掘领域中常用的无监督学习算法，MLlib中聚类的算法目前有4种，其中最常用的是Kmeans算法，在文本分类中应用较为广泛。高斯混合聚类和隐狄利克雷聚类在特定场合有特定的使用，本章将分别研究它们的算法和应用。



### 聚类与分类

聚类与分类是数据挖掘中常用的两个概念，它们的算法和计算方式有所交叉和区别。一般来说分类是指有监督的学习，即

- 要分类的样本是有标记的，类别是已知的；
- 聚类是指无监督的学习，样本没有标记，根据某种相似度度量把样本聚为k类。



#### 分类

MLlib中分类的种类很多，例如前面介绍的`决策树、贝叶斯、SVM`等都是常用的分类方法，它们的用法千差万别，对数据的要求不同，应用场景也存在不同，目前来说还没有一种能够适合于各种属性和要求的数据模型。



在前面的学习中，还有一种称为回归。回归于分类的区别在于其输出值的不同。一般情况下，

- 分类的输出是离散化的一个数据类别，
- 而回归输出的结果是一个连续值。



#### 聚类

聚类，顾名思义就是把一组对象划分成若干类，并且每个类中对象之间的相似度较高，不同类中对象之间相似度较低或差异明显。聚类是无监督学习的一种。

聚类的`目的`是分析出相同特性的数据，或样本之间能够具有一定的相似性，即每个不同的数据或样本可以被一个统一的形式描述出来，而不同的聚类群体之间则没有此项特性。



#### 区别

聚类与分类有着本质的区别，一个属于无监督学习，而一个属于有监督学习。==监督学习的意思是指==，有着特定的目标或者明确的区别，即人为可分辨。无监督学习则没有特定的规则和区别。

聚类与分类的不同之处在于，聚类算法在工作前并不知道结果如何，不会知道最终将数据集或样本划分成多少个聚类集，每个聚类集之间的数据有何种规则。聚类的目的在于发现数据或样本属性之间的规律，可以通过何种函数关系式进行表示。
聚类的要求是统一聚类集之间相似性最大，而不同聚类集之间相似性最小。MLlib中常用的聚类方法主要是Kmeans、高斯混合聚类和隐狄利克雷等，这些都将在本章中详细讲解。



### Kmeans算法

K-means算法是最为经典的基于划分的聚类方法，是十大经典数据挖掘算法之一。K-means算法的==基本思想==是：

> 在算法开始时随机给定若干（K）个中心，按照最近距离原则将样本点分配到各个中心点，之后按平均法计算聚类集的中心点位置，从而重新确定新的中心点位置。这样不断地迭代下去直至聚类集内的样本满足阈值为止。

Kmeans由于其算法设计的一些基本理念，在对数据处理时效率不高，MLlib充分利用了Spark框架的分布式计算的便捷性，从而提高了运算效率。

| ![image-20201011214838660](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201011214838660.png) | ![image-20201011214902660](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201011214902660.png) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
|                                                              |                                                              |

图9-1与图9-2不同之处在于其构成的分类不同，这也是根据Kmeans的算法基础理论所决定。若初始随机选择的初始点不同，则可能随机获得的最终结果也千差万别。





















