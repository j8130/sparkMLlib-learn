# Spark MLlib学习笔记

Spark的MLlib库基本上已经过时，现在主流的是ml库，所以玩玩就好。

一些算法可以参考github项目 https://github.com/scutan90/DeepLearning-500-questions

- spark官方推荐使用ml, 因为ml功能更全面更灵活，未来会主要支持ml，mllib很有可能会被废弃(据说可能是在spark3.0中deprecated）。
- ml中无论是什么模型，都提供了统一的算法操作接口，比如模型训练都是`fit`；不像mllib中不同模型会有各种各样的`trainXXX`。
- mllib在spark2.0之后进入`维护状态`, 这个状态通常只修复BUG不增加新功能。

## reduceByKey和groupByKey区别

先上结论

> 在对大数据进行复杂计算时，reduceByKey优于groupByKey。
>
> 另外，如果仅仅是group处理，那么以下函数应该优先于 groupByKey ：
> 　　（1）、combineByKey 组合数据，但是组合之后的数据类型与输入时值的类型不一样。
> 　　（2）、foldByKey合并每一个 key 的所有值，在级联函数和“零值”中使用。





reduceByKey用于对每个key对应的多个value进行merge操作，最重要的是它能够在本地先进行merge操作，并且merge操作可以通过函数自定义。

groupByKey也是对每个key进行操作，但只生成一个sequence。需要特别注意“Note”中的话，它告诉我们：如果需要对sequence进行aggregation操作（注意，groupByKey本身不能自定义操作函数），那么，选择reduceByKey/aggregateByKey更好。这是因为groupByKey不能自定义函数，我们需要先用groupByKey生成RDD，然后才能对此RDD通过map进行自定义函数操作。



用法（结果一样，内部处理方式不同）

~~~scala
val words = Array("one", "two", "two", "three", "three", "three")  
  
val wordPairsRDD = sc.parallelize(words).map(word => (word, 1))  

val wordCountsWithReduce = wordPairsRDD.reduceByKey(_ + _)  
val wordCountsWithGroup = wordPairsRDD.groupByKey().map(t => (t._1, t._2.sum))  
~~~



当采用reduceByKeyt时，Spark可以在每个分区移动数据之前将待输出数据与一个共用的key结合。借助下图可以理解在reduceByKey里究竟发生了什么。 注意在数据对被搬移前同一机器上同样的key是怎样被组合的(reduceByKey中的lamdba函数)。然后lamdba函数在每个区上被再次调用来将所有值reduce成一个最终结果。整个过程如下：



![image-20201007171314279](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201007171314279.png)



当采用groupByKey时，由于它不接收函数，spark只能先将所有的键值对(key-value pair)都移动，这样的后果是集群节点之间的开销很大，导致传输延时。整个过程如下：

![image-20201007171355789](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201007171355789.png)







## RDD

RDD可以看成是一个简单的“数组”，对其进行操作只需要调用有限的数组中的方法即可。与一般数组的区别在于：RDD是分布式存储，可以更好地利用现有的云数据平台，并在内存中运行（省去大量与硬盘的IO）。本质是存储在不同节点计算机中的数据集



在Spark中，弹性指的是数据的存储方式，即数据在节点中进行存储的时候，既可以使用内存也可以使用硬盘；弹性还有一层意思是，RDD具有很强的容错性，具体指Spark在运行计算的过程中，不会因为某个节点错误而使得整个任务失败。不同节点并发运行的数据，如果在某一个节点发生错误时，RDD会自动将其在不同的节点中重试。



### RDD工作原理

RDD可以将其看成一个分布在不同节点中的分布式数据集，并将数据以数据块（Block）的形式存储在各个节点的计算机中，如下图所示



![image-20201007175229211](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201007175229211.png)



从上图可以看到，每个Block Master 管理着若干个BlockSlave，而每个BlockSlaver又管理着若干个BlockNode。当BlockSlave获得了每个Node节点的地址，又会反向BlockMaster注册每个Node的基本信息，形成分层管理。

而对于某个节点中存储的数据，如果使用频率较多，则BlockMaster会将其缓存在自己的内存中，这样如果以后需要调用这些数据，则可以直接从BlockMaster中读取。对于不再使用的数据，BlockMaster会向BlockSlave发送一组命令予以销毁。



> tip
>
> 窄依赖便于在单一节点上按次序执行任务，使任务可控。宽依赖更多的是考虑任务的交互和容错性。





## chapter04 MLlib 基本概念

RDD将存储数据转化成向量和矩阵的形式进行存储和计算，这样将数据定量化表示，能更准确地整理和分析结果



### MLlib 基本数据类型

MLlib支持较多的数据格式，从最基本的Spark数据集RDD到部署在集群中的向量和矩阵

MLlib目前只支持整数与浮点数。其他类型均不支持，这与MLlib用途有关，主要用于数值计算



MLlib基本数据类型

| 类型名称           | 释义                                                 |
| ------------------ | ---------------------------------------------------- |
| Local vector       | 本地向量集。主要向spark提供一组可进行操作的数据集和  |
| Labeled point      | 向量标签。让用户能够分类不同的数据集和               |
| Local matrix       | 本地矩阵。将数据集合以矩阵形式存储在本地计算机中     |
| Distributed matrix | 分布式矩阵。将数据集合以矩阵形式存储在分布式计算机中 |



### 两组数据相关系数计算

相关系数是一种用来反映变量之间相关关系密切程度的统计指标，在现实中一般用于对两组数据的`拟合`和`相似程度`进行定量化分析。常用的一般是皮尔逊相关系数，MLlib中默认的相关系数求法也是使用皮尔逊相关系数发。斯皮尔曼相关系数用的比较少，但是其能够比较好地反映不同数据集的趋势程度。



皮尔逊相关系数计算公式
$$
\rho_{xy} = 
\frac{
	\sum(x-\overline{x})(y-\overline{y})
}{
	\sqrt{\sum(x-\overline{x}){2}  \sum(y-\overline{y}){2}}
}
$$
可以看做是两组数据的向量夹角的余弦，用来描述两组数据的分开程度。



斯皮尔曼相关系数计算公式
$$
\rho_{xy} = 1-
\frac{
	6\sum(x_i-y_i){2}
}{
	n(n^{2}-1)
}
$$

> n为数据个数



总结：

皮尔逊相关系数代表两组数据的余弦分开程度，表示随着数据量的增加，两组数据差别将增大

斯皮尔曼相关系数更注重两组数据的拟合程度，即两组数据随着数据量的增加而增长曲线不变



## chapter05 协同过滤算法



协同过滤算法是最常用的推荐算法，主要有两种具体形式：基于用户的推荐算法 和 基于物品的推荐算法。  推荐算法的基础是基于两个对象之间的相关性。



### 协同过滤

协同过滤算法是一种基于群体用户或者物品的典型推荐算法，是目前常用推荐算法中最常用和最经典的算法。

协同过滤算法主要有两种

- 一是通过考察具有相同爱好的用户对相同物品的评分标准进行计算
- 而是考察具有相同特质的物品从而推荐给选择了某件物品的用户

总体来说，协同过滤算法是建立在基于某种物品和用户之间相互关联的数据关系之上。



#### 基于用户的推荐

一句话概括：给推荐`志趣相投`的，俩人有两个物品都喜欢，则第一个人喜欢的另外一个物品，第二个人可能也喜欢

#### 基于物品的推荐

给推荐`物以类聚`的，以已有物品为线索去进行相似度计算从而推荐给特定的目标用户。



#### 协同过滤算法的不足

> 基于用户的推荐算法，针对某些热点物品的处理不够准确，对于一些常用的物品推荐，其计算结果往往排在推荐的首位，而这样的推荐没有实际应用意义。同时基于用户的推荐算法，往往数据量较为庞大，计算费事，由于热点的存在准确度也很成问题
>
> 基于物品的推荐算法相对于基于用户的推荐算法，其数据量小很多，可以较为容易地生成推荐值，但是其存在推荐同样（同类型）物品的问题。例如，用户购买了某件商品，那么推荐系统可能会继续推荐相同类型的商品给用户，用户在购买一件商品后，绝对不会再购买同类型的商品，这样的推荐是失败的。

### 相似度度量

协同过滤的最重要部分是相似度求得



#### 欧几里得距离

三维空间两个点的真实距离。

由于在欧几里得距离相似度计算中，最终数值的大小与相似度成反比，因此在实际应用场景中常常使用欧几里得距离的倒数作为相似度值，即 d/d+1 作为近似值

> 就是计算勾股定理的长边

#### 余弦相似度

与欧几里得距离相似，余弦相似度也将特定目标，即物品或者用户作为坐标上的点，但不是坐标原点。基于此与特定的被计算目标进行夹角计算。

> 就是计算夹角



#### 欧几里得相似度和余弦相似度比较

欧几里得相似度是以目标绝对距离为衡量的标准，而余弦相似度是以目标差异的大小作为衡量标准。

![image-20201008161851096](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201008161851096.png)

从上图可以看出，欧几里得相似度注重目标之间的差异，与目标在空间中的位置直接相关。而余弦相似度是不同目标在空间中的夹角，更加表现在前进趋势上的差异。

一般来说欧几里得用来表现不同目标的绝对差异性，分析目标之间的相似度与差异情况。余弦相似度更多的是对目标从方向趋势上区分，对特定坐标数字不敏感。



### ALS算法 - 交替最小二乘法

交替最小二乘法是统计分析中最常用的逼近计算中的一种算法，其交替计算结果使得最终结果尽可能地逼近真实结果。

#### 最小二乘法（LS算法）

LS算法是ALS算法的基础，LS算法是一种数学优化技术。通过最小化误差的平方和寻找数据的最佳函数配比。利用最小二乘法可以简便地求得未知数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。最小二乘法还可用于曲线拟合。其他一些优化问题也可通过最小化能量或最大化熵用最小二乘法来表达。

LS算法原理

![image-20201008164026358](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201008164026358.png)

若干个点分布在向量空间中，如果希望找出一条直线和这些点达到最佳匹配。最简单的一个方法就是希望这些点到直线的距离值最小。

f(x) 是直接拟合公式，即目标函数
$$
f(x)=ax+b
$$

$$
\delta=\sum(f(x_i)-y_i)^2
$$





#### 交替最小二乘法（ALS算法）

比较难，详细理解还需要再百度

假设基于用户名、物品表的用户评分矩阵可以被分解成2个较为小型化的矩阵，即矩阵 U 和矩阵 V 。因此可以将原始矩阵近似表示为：W=U*V

这里U和V分别表示用户和物品的矩阵，在MLlib的ALS算法中，首先对U或者V矩阵随机化生成，之后固定某一个特定对象，去求另一个未随机化的矩阵对象。然后利用被求取的矩阵对象去求随机化矩阵对象。最后两个对象相互迭代计算，求取与实际矩阵差异达到程序设定的最下阈值位置。如下所示

![image-20201008171214175](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201008171214175.png)



## chapter06 线性回归

> 解决了对某个具体数据进行预测的问题。

回归分析是一种用来确定两种或两种以上变量间相互依赖的定量关系的统计分析方法。分析可以按以下要素分类

- 按照涉及的自变量的多少，分为回归和多重回归分析
- 按照自变量的多少，可分为一元回归分析和多元回归分析
- 按照自变量与因变量之间的关系类型，可分为线性回归分析和非线性回归分析

> 在回归分析中，只包括一个自变量和一个因变量，且二者关系可用一条直线近似表示，称为一元线性回归分析。
>
> 如果回归分析中包括两个或两个以上的自变量，且因变量与自变量之间是线性关系，则称为多重线性回归分析。



### 随机梯度下降算法

机器学习中回归算法有 神经网络回归算法、蚁群回归算法、支持向量机回归算法等。MLlib中使用的是较为经典的随机梯度下降算法，它充分利用Spark框架的迭代计算特性，通过不停地判断和选择当前目标下的最优路径，从而能够在最短路径下达到最优的结果，继而提过大数据计算效率



道士下山

![image-20201008173958281](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201008173958281.png)

> 简单来说，梯度下降就是从山顶找一条最短的路走到山脚最低的地方。但是因为选择方向的原因，我们找到的的最低点可能不是真正的最低点。如图所示，黑线标注的路线所指的方向并不是真正的地方。
>
> 总结起来就一句话：随机选择一个方向，然后每次迈步都选择最陡的方向，直到这个方向上能达到的最低点。



梯度下降算法模型

![image-20201008174753527](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201008174753527.png)



### 回归的过拟合

详细理解需要百度

消除回归算法中的误差，通过增加系数等方法修正过拟合。

消除过拟合的过程称为回归的正则化。正则化使用较多的一般有两种方法，lasso回归（L1回归）和岭回归（L2回归），其目的是通过对最小二乘法估计加入惩罚因素，使某些系数的估计为0。



### 线性回归

MLlib中，线性回归的基本数据是严格按照数据格式进行设置。例如，如果想求得公式
$$
y=2x_1+3x_2
$$
的系数，那么需要在数据基础中设置2个x值，并且在其前部设置y值

~~~markdown
1,0 1
2,0 2
3,0 3
5,1 4
7,6 1
9,4 5
6,3 3

逗号前面的数值是根据不同的数据求出的结果值，而每个系数x值依次地被排列在其后，这些就是数据收集的规则：Y=a+bX

~~~



### 回归曲线的验证

使用均方误差 MSE

## chapter07 分类

分类算法又称分类器，是数据挖掘和机器学习领域中的一个非常重要的分支和方向。

### 逻辑回归

逻辑回归不是回归算法，是`分类算法`



- 逻辑回归和线性回归类似，但它不属于回归分析家族，差异主要在于变量不同，因此其解法和生成的曲线也不尽相同。
- 逻辑回归一般用于`对某些数据或事物的归属及可能性进行评估`。比如，探索某疾病的危险因素，根据危险因素预测某疾病发生的概率。

例如：

> 想探讨胃癌发生的危险因素，可以选择两组人群，一组是胃癌组，一组是非胃癌组，两组人群肯定有不同的体征和生活方式等。这里的因变量就是是否胃癌，即"是"或"否"，为两分类变量，自变量既可以包括很多了，例如年龄、性别、饮食习惯、幽门螺杆菌感染等。自变量可以是连续的，也可以是分类的。
>
> 在上面胃癌的例子中，尽管收集了各种变量因素，但是在胃癌被确诊前，任何人都无法对某人是否将来会诊断出胃癌做出断言，而只能说“有可能”患有胃癌。这个就是逻辑回归。他不直接告诉你结果的具体数据而会告诉你可能性在哪里。



~~~markdown
1|2
1|3
1|4
1|5
1|6
0|7
0|8
0|9
0|10
0|11
~~~

分隔符用以标示分类结果和数据组。用y和x表示，则y为 0或1，x为数据集中数据的特征向量。

逻辑回归的公式
$$
f(x)=
\frac{
	1
}{
	1+exp(-\theta^Tx)
}
$$
与线性回归相同， 这里的$$\theta$$ 是逻辑回归的参数，即逻辑系数，如果再将其进一步变形，使其能够反映二元分类问题的公式为：
$$
f(y=1|x,\theta)=
\frac{
	1
}{
	1+exp(-\theta^Tx)
}
$$
这里y值是由已有的数据集中数据和$$\theta$$共同决定。实际上这个公式求计算是在满足一定条件下，最终取值的对数机滤，即由数据集的可能性的比值的对数变换得到。通过公式表现为：
$$
log(x)=
ln(
\frac{
	f(y=1|x,\theta)
}{
	f(y=0|x,\theta)
}
)=
\theta_0+\theta_1x_1+\theta_2x_2+...++\theta_nx_n
$$
通过逻辑回归的倒推公式可看出，最终逻辑回归的计算可以转化成数据集的特征向量与系数$$\theta$$共同完成，然后求得其`加权和`作为最终的判断结果。



### 支持向量机

支持向量机是数据挖掘中的一个新方法，可以非常成功地处理回归（时间序列分析）和模式识别（分类问题，判别分析）等诸多问题，并可推广到预测和综合评价等领域。

MLlib对支持向量机算法有较好的支持，可以用来解决一般线性回归和逻辑回归不好处理的数据分类内容，结果验证其准确性较好。



<img src="https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201011153954199.png" alt="image-20201011153954199" style="zoom:67%;" />



两条虚线被成为支持向量，通过支持向量从而获得分类平面的方法，称为支持向量机。这个是二维平面的，有高维空间的支持向量机，具体学习要百度。



### 朴素贝叶斯



朴素贝叶斯是基于假设的先验概率、给定假设下观察到不同数据的概率，以及观察到的数据本身而得出的。其方法为，将关于未知的参数的先验信息与样本信息综合，再根据贝叶斯公式，得出后验信息，然后根据后验信息去推断未知参数的方法。



https://zh.wikipedia.org/wiki/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8

应用场景

- 通过一些测量的特征，包括身高、体重、脚的尺寸，判定一个人是男性还是女性。
- 考虑一个基于内容的文本分类问题，例如判断邮件是否为垃圾邮件。

具体学习看文本开始的github项目



## chapter08 决策树与保序回归

决策树是分类算法的一个分支，决策树是一种监管学习，所谓监管学习，就是给定一堆样本，每个样本都有一组属性和一个类别，这些类别是事先确定的，那么通过学习得到一个分类器，这个分类器能够对新出现的对象给出正确的分类。目前决策树是分类算法中应用较多的算法之一，其原理是从一组无序无规律的因素中归纳总结出符合要求的分类规则。

随机雨林顾名思义，是决策树的一种大规模应用形式，其充分利用了大规模计算机并发计算的优势，可以对数据进行并行的处理和归纳分类。

任何一个只要符合`k-v`模式的分类数据都可以根据决策树进行推断。



### 决策树

MLlib中决策树是一种典型的回归算法。与线性回归和逻辑回归不同，它在处理数据缺失和非线性方面有较多的应用价值。

决策树是在已知各种情况发生概率的基础上，通过构成决策树来求取净现值的期望值大于等于0的概率，评价项目风险，判断其可行性的决策分析方法，是直观运用概率分析的一种图解法。



![image-20201011165019100](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201011165019100.png)

> 每个分支和树叶代表一个分支向量，每个节点代表一个输出结果或分类
>
> 决策树用来预测的对象是固定的，从根到叶子节点的一条特定路线就是一个分类规则，决定这一个分类算法和结果。
>
> 决策树的生成算法是从根部开始，输入一系列带有标签分类的示例（向量），从而构造出一些列决策节点。这些节点又称为逻辑判断，表示该属性的某个分支（属性），供下一步继续判定，一般有几个分支就有几条有向有向的线作为类别标记



#### 信息熵

信息熵是决策树的理论基础

信息熵，指的是对事件中不确定的信息的度量。一个事件或者属性中，其信息熵越大，其含有的不确定信息越大，对数据分析的计算也越有益。因此，信息熵的选择总是选择当前事件中拥有最高信息熵的那个属性作为待测属性。



#### ID3算法

ID3算法是基于信息熵的一种经典决策树构建算法，ID3是一种贪心算法。以信息熵的下降速度为选取测试属性的标准，即在每个节点选取还尚未被用来划分的、具有最高信息增益的属性作为划分标准，然后继续这个过程，直到生成的决策树能完美分类训练样例。

可以说，ID3算法核心就是信息增益的计算。



#### 随机雨林与梯度提升算法

随机雨林与梯度提升是==利用分布式并发处理系统构建的并发式决策树==。

![image-20201011180007947](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201011180007947.png)





> 雨林是若干个树的集合。随机雨林从名称上看，就是若干个决策树所组成的一个决策树林。在MLlib中，随机雨林中的每一棵树都被分配到不同的节点上进行并行计算，或者在一些特定的条件下，单独的每一
> 棵决策树都可以并行化计算，每一棵决策树之间是没有相关性的。
>
> 当随机雨林在运行的时候，每当一个新的数据被传输到系统中，则会由随机雨林中的每一棵决策树同时进行处理。如果结果是一个连续常数，则对每一棵的结果取得平均值作为结果。如果是非连续结果，则选
> 择所有决策树计算最多的一项作为数据计算的结果。



**梯度提升算法**

梯度提升算法的思想类似于前面学习线性回归时的随机梯度下降算法。一个模型中有若干个属性值构成，每个属性值在开始训练时具有相同的权重，之后不断地将模型计算结果与真实值进行比较。如果出错则
降低在特定方向的损失。这段话比较拗口，由于梯度提升算法较为复杂，这里笔者只简单地提及其基本原理，其数学基础和公式请感兴趣的读者参考相关资料学习。



### 保序回归

相对于决策树，保序回归的应用范围没有决策树算法那么广泛，但是在一般应用时，特别是数据处理较为庞大的情况下，采用保序回归做回归分析可以极大地节省资源，从而提高计算效率。



保序回归是数理统计中的一种回归计算方法，它在有约束的条件下，对数据进行回归处理。其处理方法是对数据列的均值进行处理从而获得一个回归序列。



> 我们用一个简单的例子描述保序回归。例如，给定一个无序数据集，要求预测数据集中某个位置数值的大小。但是由于系统内容空间有限，数据集过大，因此有个约束条件，就是在计算时不能够对数据进行排序或者无法进行排序，但是可以对数据进行修改。
>
> 保序回归的思想是对数据进行均值排序，从数据集的第一个数开始，如果下一个数出现乱序，即与设定的顺序不符，则从乱序的数据开始逐个开始求得平均值，直到求得的平均值与下一个数据比较不成为乱序为止。例如一个数据集：
> `{ 1 , 3 , 2 , 4 , 5 }`
>
> 要求将其按保序回归由小到大进行排列。首先观察第一个数是1，可以不做变动继续存放。第二个是3，仍旧不需要变动。第三个数是2，则属于乱序从而需要对其重新计算。
>
> 第三个数是乱序，需要从其开始计算，提取数据2和下一个数据4，计算得到其平均值为3，因此可获得一个新的数据集：
> `{ 1 , 3 , 3 , 3 , 5 }`
> 继续观察下一个数值，符合排列要求到达最后一个值，从而完成保序回归。



## chapter09 聚类

> 从数据挖掘的角度来看，聚类算法是无监督学习算法的一种。无监督学习指的是没有预先的定义和标记，由算法自行完成分类和聚合，是一种探索性的分析。聚类算法从本身的算法出发，自动探索并对数据进行处理，往往因为处理时间的不同和循环迭代的次数不同，以及方法的先后顺序从而得到不同的聚类结论。不同的工作人员对同一组数据进行处理，结果也不近相同。
> “物以类聚，人以群分”，聚类方法是较为常见的对数据进行处理的方法，也是一种常见的数据挖掘算法的预处理过程。在MLlib中聚类方法还有更多的算法有待开发，读者可以先以已有的算法为基础，掌握它们的基本理论和用法，也可以自行编写相应的代码，从而获得更合适的算法。



聚类是一种数据挖掘领域中常用的无监督学习算法，MLlib中聚类的算法目前有4种，其中最常用的是Kmeans算法，在文本分类中应用较为广泛。高斯混合聚类和隐狄利克雷聚类在特定场合有特定的使用，本章将分别研究它们的算法和应用。



### 聚类与分类

聚类与分类是数据挖掘中常用的两个概念，它们的算法和计算方式有所交叉和区别。一般来说分类是指有监督的学习，即

- 要分类的样本是有标记的，类别是已知的；
- 聚类是指无监督的学习，样本没有标记，根据某种相似度度量把样本聚为k类。



#### 分类

MLlib中分类的种类很多，例如前面介绍的`决策树、贝叶斯、SVM`等都是常用的分类方法，它们的用法千差万别，对数据的要求不同，应用场景也存在不同，目前来说还没有一种能够适合于各种属性和要求的数据模型。



在前面的学习中，还有一种称为回归。回归于分类的区别在于其输出值的不同。一般情况下，

- 分类的输出是离散化的一个数据类别，
- 而回归输出的结果是一个连续值。



#### 聚类

聚类，顾名思义就是把一组对象划分成若干类，并且每个类中对象之间的相似度较高，不同类中对象之间相似度较低或差异明显。聚类是无监督学习的一种。

聚类的`目的`是分析出相同特性的数据，或样本之间能够具有一定的相似性，即每个不同的数据或样本可以被一个统一的形式描述出来，而不同的聚类群体之间则没有此项特性。



#### 区别

聚类与分类有着本质的区别，一个属于无监督学习，而一个属于有监督学习。==监督学习的意思是指==，有着特定的目标或者明确的区别，即人为可分辨。无监督学习则没有特定的规则和区别。

聚类与分类的不同之处在于，聚类算法在工作前并不知道结果如何，不会知道最终将数据集或样本划分成多少个聚类集，每个聚类集之间的数据有何种规则。聚类的目的在于发现数据或样本属性之间的规律，可以通过何种函数关系式进行表示。
聚类的要求是统一聚类集之间相似性最大，而不同聚类集之间相似性最小。MLlib中常用的聚类方法主要是Kmeans、高斯混合聚类和隐狄利克雷等，这些都将在本章中详细讲解。



### Kmeans算法

K-means算法是最为经典的基于划分的聚类方法，是十大经典数据挖掘算法之一。K-means算法的==基本思想==是：

> 在算法开始时随机给定若干（K）个中心，按照最近距离原则将样本点分配到各个中心点，之后按平均法计算聚类集的中心点位置，从而重新确定新的中心点位置。这样不断地迭代下去直至聚类集内的样本满足阈值为止。

Kmeans由于其算法设计的一些基本理念，在对数据处理时效率不高，MLlib充分利用了Spark框架的分布式计算的便捷性，从而提高了运算效率。

| ![image-20201011214838660](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201011214838660.png) | ![image-20201011214902660](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201011214902660.png) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
|                                                              |                                                              |

图9-1与图9-2不同之处在于其构成的分类不同，这也是根据Kmeans的算法基础理论所决定。若初始随机选择的初始点不同，则可能随机获得的最终结果也千差万别。



### 高斯混合聚类



#### 高斯分布

为了更好地理解高斯分布，读者需要注意，高斯分布有一个更为常用和有名的特例——正态分布。

高斯分布是指 若随机变量X服从一个数学期望为`μ`、方差为`σ^2`的高斯分布，记为`N(μ，σ^2)`。它的概率密度函数为高斯分布的期望值μ决定了分布的位置，标准差σ决定了分布的幅度。因其曲线呈钟形，人们又常称之为钟形曲线。
我们通常所说的标准高斯分布是μ=0、σ=1的正态分布，其形状如图

![image-20201011220202682](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201011220202682.png)

高斯分布的数学表达公式如下：
$$
f(x)=
\frac{
	1
}{
	\sqrt{2\pi}\theta
}
exp(-
\frac{
	(x-\mu)^2
}{
	2\theta^2
}
)
$$


这里μ和σ都是用以表示分布的位置。顺便提一句，当μ=0、σ=1时，此时高斯分布成为一个经典的分布形式，即正态分布。
$$
f(x)=
\frac{
	1
}{
	\sqrt{2\pi}
}
exp(-
\frac{
	x^2
}{
	2
}
)
$$


高斯分布在应用上常用于图像处理、数据归纳和模式识别等方面，对图像噪声的提取、特征分布的鉴定等方面有重要的功能。此外，高斯分布也用于对图像的处理，例如Photoshop软件中有一项专门的功能称为高斯过滤。
==以高斯分布为基础的单高斯分布聚类模型==，其==原理==就是考察已有数据建立一个分布模型，之后通过带入样本数据计算其值是否在一个阈值范围之内。

换句话说，对于每个样本数据考察期与先构建的高斯分布模型的匹配程度，例如当一个数据向量在一个高斯分布的模型计算阈值以内，则认为它与高斯分布相匹配。如果不符合阈值则认为不属于此模型的聚类。

一维高斯分布模型在上一节中已经介绍，下面主要介绍多维高斯分布模型。==多维高斯分布模型公式==如下：
$$
G(x,\mu,\theta)=
\frac{
	1
}{
	\sqrt{2\pi}
}
exp(-
\frac{
	(x-\mu)^{n-1}
}{
	2\theta
}
)
$$

> 其中x是一个样本数据，μ和σ分别为样本的期望和方差。通过带入计算很容易判断样本x是否属于整体模型。

高斯分布模型可以通过训练已有的数据得到，并通过更新减少人为干扰，从而实现自动对数据进行聚类计算。



#### 混合高斯分类

混合高斯模型主要是为了解决单高斯模型对混合的数据聚合不理想的情况。

图9-5演示了一个很明显的情况，对于过度重叠在一起的数据，单高斯模型无法对其进行严谨区分，为了解决这个问题，引入了混合高斯模型。

![image-20201011221305167](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201011221305167.png)



混合高斯模型的原理用简单的一句话表述为，==任何样本的聚类都可以使用多个单高斯分布模型来表示==。其公式如下：
$$
Pr(x)=
\sum\pi G(x,\mu,\theta)
$$
公式中G(x,μ,σ)是混合高斯模型的聚类核心，我们需要做的就是在样本数据已知的情况下，训练获得模型参数，这里使用的是极大似然估计。具体本书就不做介绍了，请有兴趣的读者自行学习。



### 快速迭代聚类

快速迭代是聚类方法的一种，但是其基础理论比较难。

**理论基础**

快速迭代聚类是谱聚类的一种。谱聚类是最近聚类研究的一个热点问题，是建立在图论理论上的一种新的聚类方法。快速迭代聚类的基本原理是使用含有权重的无向线将样本数据连接在一张无向图中，之后按相似度进行划分，使得划分后的子图内部具有最大的相似度而不同子图具有最小的相似度从而达到聚类的效果。

图9-6演示了对数据集进行切分聚类的方法。与Kmeans类似，这里的聚类也属于无监督学习方法，因此其切分可以不同，并没有一个特定的限制。

![image-20201011222252520](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201011222252520.png)



每个点之间的距离由点之间的相似度计算获得，一般采用的是欧式距离表示。另外还有余弦相似度和高斯核函数相似度表示，读者查阅相关材料可自行研究。



谱聚类基本原理就是利用计算得到的样本相似度，组成一个相似度矩阵进行聚类计算。



## chapter10 MLlib中关联规则

关联规则是数据挖掘领域最为活跃和使用范围最广的研究方法。

关联规则是研究不同类型的物品相互之间关联关系的规则，它最早是针对沃尔玛超市的购物数据分析诞生的，可以用来指导超市进行购销安排。之后应用于其他领域，例如医学病例的共同特征挖掘以及网络入侵检测等，都可以使用关联规则进行处理。



### Apriori频繁项集算法

> **啤酒与尿布**
>
> “啤酒与尿布”是一个神奇的故事。20世纪沃尔玛超市的营销人员在对商品销售情况进行统计的时候发现，在某些特定的日子，“啤酒”和“尿布”这两样看起来没有任何相关性的商品会经常性地出现在同一份购物清单上，这种奇怪的现象引起了沃尔玛的注意。经过追踪调查后发现，在美国传统家庭中，一般是由母亲在家照顾新生婴儿，而父亲外出工作。经
> 常性地在父亲结束工作后会进入超市采购日常用品，而此时往往有的父亲在给婴儿购买尿布时给自己顺带买点啤酒。这样使得看起来没有任何相关性的商品被紧密的联系在一起。沃尔玛发现了这个现象后，开始尝试将啤酒与尿布摆放在尽可能远的地方，连接通道的货架上摆放着能够吸引年轻父亲的一些具有吸引力的商品，从而使得他们能够尽可能多地购物。
>
> 当然这只是一个简单的关联关系例子。但是作为大数据分析和处理人员，通过对购物清单进行分析从而找出商品在购买时的关联关系，进而研究客户的购买行为，是一个非常重要的工作技能。



百科解释：

Apriori算法“是一种挖掘关联规则的频繁项集算法，其核心思想是通过候选集生成和情节的向下封闭检测两个阶段来挖掘频繁项集，而且算法已经被广泛地应用到商业、网络安全等各个领域



**以啤酒与尿布例子讲解基本原理**

五份超市商品购买清单，其中每一单行代表一个顾客购买的物品清单，简单起见这里省略了购买物品数量。

![image-20201011225226551](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201011225226551.png)

为了简化理论说明起见，首先定义两个相互独立的集合X和Y，假设X和Y之间有一定的关联性，即相互之间存在关联规则。而关联规则的表示使用支持度和置信度来说明。



支持度表示X和Y中的项在同一情况下出现的次数。支持度（Support）的公式是：
$$
Support(A->B)=P(A\cup B)
$$
支持度揭示了A与B同时出现的概率。如果A与B同时出现的概率小，说明A与B的关系不大；如果A与B同时出现非常频繁，则说明A与B总是相关的。

例如在表10-1中，啤酒与尿布的同时出现的次数为3，而全部清单数为5，则可以判定啤酒与尿布的支持度为3/5。

置信度表示X和Y在一定条件下出现的概率。置信度（Confidence）的公式是：
$$
Conffidence(A->B)=P(A | B)
$$
置信度揭示了A出现时，B是否也会出现或有多大的概率出现。如果置信度为100％，则A和B可以捆绑销售了。如果置信度太低，则说明A的出现与B是否出现关系不大。
例如，在表10-1中，啤酒与尿布的同时出现的次数为3，而啤酒出现的次数为4，则可以判定啤酒与尿布的置信度为3/4。

Apriori算法是由两部分组成，即A和priori组合而成。含义是指每一项的计算是在前面项的基础上计算得到，即需要一个先验计数。具体如图所示。

![image-20201011225732826](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201011225732826.png)

从图可以看到，首先计算所需要的项的支持个数，抛弃数据支持个数过少的项。之后以此为基础相互组合重新计算个数。

tip:

> Apriori算法属于候选消除算法，是一个生成候选集，消除不满足条件的候选集，并不断循环直到不再产生候选集的过程。



### FP-growth算法

FP-growth在算法上较为容易理解，在程序编写上有一定难度。



#### Apriori算法的局限性

> Apriori算法是关联算法中比较经典的算法。它便于理解和程序代码实现，因此在一般数据处理和数据挖掘中应用非常广泛，但是由于它在算法设计上具有很大的局限性，并不能较为合适地处理大数据。
>
> 最主要的是Apriori使用的是A和priori这一特性来生成频繁项候选集，这样做的好处是在单机的情况下，可以对频繁项集进行压缩处理，从而在有限的内存情况下最大限度地提高了运算效率。但是这样做带来好处的同时还存在着两个主要问题：
>
> 第一问题就是会产生较多的小频繁项，小频繁项集过多使得数据在进行计算处理的时候效率极大地降低，从而使得复杂度以指数形式增长，降低了Apriori整体效率。
> 第二问题是由于频繁项集的处理需要多次扫描原样本数据库，而一般情况下IO的处理需要消耗大量的处理时间，从而算法在计算的过程中消耗大量的资源在数据的读取上。



#### FP-growth算法

基于以上Apriori算法的不足，一个新的关联算法被提出，即FP树算法（FP-Growth）。这个算法试图解决多次扫描数据库从而带来的大量小频繁项集的问题。这个算法在理论上只对数据库进行两次扫描，直接压缩数据库生成一个频繁模式树从而形成关联规则。

在具体过程上，FP树的算法主要由两大步骤完成：
（1）利用数据库中的已有样本数据构建FP树；
（2）建立频繁项集规则。

为了更好地解释FP树的建立规则，我们使用表10-1提供的数据清单为例进行讲解。
FP树算法的第一步就是扫描样本数据库，将样本按递减规则排序，删除小于最小支持度的样本数。结果如下：

~~~markdown
果汁 4 
鸡肉 4 
啤酒 4 
尿布 3
~~~

这里使用最小支持度为3，得到以上计数结果。之后第二步重新扫描数据库，并将样本按上标的支持度数据排列，结果如表10-2所示

![image-20201011231717627](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201011231717627.png)

表10-2是已经对数据进行的重新排序，从T5的顺序可以看到，由原来的“鸡肉、果汁、啤酒、可乐”被重排为“果汁、鸡肉、啤酒”，这是第二次扫描数据库也是FP树算法最后一次扫描数据库。

下面开始构建FP树，将重新生成的表10-2按顺序插入FP树中，如图10-3所示。

|                                                              | <img src="https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201011231914629.png" alt="image-20201011231914629" style="zoom:67%;" /> |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 这里需要说明的是，Root是空集，用来建立后续的FP树。之后<br/>继续插入第二条记录。如图10-4所示。 | <img src="https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201011232244142.png" alt="image-20201011232244142" style="zoom:67%;" /> |
| 此时可以看到，在新生成的树中，鸡肉的数量变成2，这样继续生<br/>成FP树，可以得到如下图10-5所示完整的FP树。 | ![image-20201011232341420](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201011232341420.png) |
| 建立对应的FP树之后，就可以开始频繁项集挖掘工程，这里采用逆<br/>向路径工程对数据进行数据归类。首先需要建立的是样本路径。如图<br/>10-6所示。 | ![image-20201011232408044](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201011232408044.png) |



这里假设需要求取“啤酒、尿布”的包含清单，则从支持度最小项开始，可以获得如下数据：

~~~markdown
尿布：1，啤酒：2，鸡肉：2 
尿布：1，啤酒：1，果汁：4
尿布：1，啤酒：1，鸡肉：1
~~~

之后在新生成的表中递归查找包含“尿布”的项，完成项目查找并计算相关置信度。



### 小结

Apriori虽然便于理解和编写程序，但是由于它要求多次扫描数据集，会带来无谓的资源损耗，因此在大数据领域其缺乏实用价值。

FP树是为了解决Apriori算法需要对数据集进行多次读取这个弊端而诞生的，它只需要读取两次数据集即可。

FP树是一个较Apriori算法而言更为轻量级算法，在求解和复杂度分析方面有着极大的优势，但是同样对于大数据而言，它的空间复杂度和时间复杂度依然较高，这点需要相关使用人员注意。

## chapter11 数据降维(线代-很难)

> SVD和PCA属于线性无监督降维

数据降维又称为维数约简，从名称上看就是降低数据的维数。目前MLlib中使用的降维方法主要有两种：奇异值分解（SVD）和主成分分析（PCA）。

由于本章的内容较为理论化，如果有的读者对其感到难以理解可以跳过本章的理论部分，直接掌握相关程序的用法即可。

### 奇异值分解（SVD）

奇异值分解，这是矩阵分解的常用方法，将一个大矩阵分解为若干个低维度的矩阵来表示是其最终目的。

可以简单地把奇异值分解理解为：一个矩阵分解成带有方向向量的矩阵相乘。即：
$$
A=UΣV^T
$$
![image-20201011233943000](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201011233943000.png)

其中的U是一个M*K阶层的矩阵，Σ是一个K*K的矩阵，而V也是一个N*K阶层的矩阵，这三个方阵相乘的结果就是形成一个近似于A的矩阵。这样做的好处是能够极大地减少矩阵的存储空间，很多数据矩阵在经过SVD处理后，其所占空间只有原先的10％左右，从而极大地提高运算效率。



### 主成分分析（PCA）

主成分分析是设法将原来众多具有一定相关性（比如P个指标）的指标，重新组合成一组新的互相无关的综合指标来代替原来的指标，从而实现数据降维的目的，这也是MLlib的处理手段之一

> def：
>
> 主成分分析（PCA）在百度百科上的解释为，“在用统计分析方法研究多变量的课题时，变量个数太多就会增加课题的复杂性。人们自然希望变量个数较少而得到的信息较多。在很多情形，变量之间是有一定的相关关系的，当两个变量之间有一定相关关系时，可以解释为这两个变量反映此课题的信息有一定的重叠。主成分分析是对于原先提出的所
> 有变量，将重复的变量（关系紧密的变量）删去，建立尽可能少的新变量，使得这些新变量是两两不相关的，而且这些新变量在反映课题的信息方面尽可能保持原有的信息。”
>
> 
>
> 设法将原来变量重新组合成一组新的互相无关的几个综合变量，同时根据实际需要从中可以取出几个较少的综合变量，以尽可能多地反映原来变量的信息的统计方法叫做主成分分析或称主分量分析，也是数学上用来降维的一种方法。



## chapter12特征提取和转换

> 与数据降维相同，特征提取和转换也是处理大数据的一种常用方法手段，其目的是创建新的能够代替原始数据的特征集，更加合理有效地展现数据的重要内容。特征提取指的是由原始数据集在一定算法操作后创建和生成的新的特征集，这种特征集能够较好地反映原始数据集的内容，同时在结构上大大简化。
> MLlib中目前使用的特征提取和转换方法主要有TF-IDF、词向量化、正则化、特征选择等，这些方法在本章中都会介绍。需要注意的，特征提取和转换算法的实际应用都要求与实际应用领域相结合，不的领域有着不同的特征提取和转换的方法，这点需要读者在学习中注意。



### TF-IDF

TF-IDF是一种较为简单的特征提取算法，它简单到任何使用者只需要一小时就可以掌握其原理。而且在实际应用领域中，TF-IDF算法作为一个经典的数据挖掘算法有着广泛的应用情景。
MLlib中使用TF-IDF算法作为文本特征提取算法，使用的数学公式较为简单，建议读者可以深入学习一下。



概括起来说，TF-IDF的一般定义如下：

- TF（Term Frequency）为词频的定义，表示为某个关键词在一个文本中出现的次数。一般认为某个特定词在当前文本中出现的次数越多，越能反映出文本特征。
- IDF（Inverse Document Frequency）为逆文本频率定义。表示为某个关键词在一个文本集中的区分能力。若某个特定关键词在文本集中出现的次数越多，则其区分能力越差。例如一些常用的介词完全没有任何区分能力，反而出现次数最多。

TF与IDF的计算公式
$$
TF=
\frac{
	某个词在文章中出现的次数
}{
	文章的总词数
}
$$

$$
IDF=log(
\frac{
	查找的文章总数
}{
	包含该词的文章数+1
})
$$


从IDF公式中可以看到，一个词如果在不同的文章中出现的较多，即较为常见，则可认为其分母越大，计算得到的IDF值越小。分母加一是为了防止分母为0造成的系统计算错误。
因此，最终获得的TF-IDF计算公式如下：
$$
TF-IDF=
TF(词频)* IDF(逆文档频率)
$$

> tip：
>
> 还需要注意的是，对于不同的文本信息，经过TF-IDF确定的关键词向量后，其中可能包含较多数目的特征关键词，因此选取不同数目的可信关键词会对结果造成一定程度的影响。一般认为，选取的关键词数目偏少，代表的信息熵不足；过多的话，则可能会给关键词向量引入较多的噪声项，降低文本信息相似度计算的准确性。



### 词向量化工具

简单地说，现实中的语言文本问题要转化为机器学习或数据挖掘的问题，第一步肯定是要找一种方法把这些符号数字化，即要将语言文本翻译成机器能够认识的语言。词向量工具就是为了解决这个翻译问题而诞生的。
MLlib中提供了词向量化的工具，其目的是在于不增加维数的前提下将大量的文本内容数字化。本节的学习可以与文本相似度距离结合在一起，以便更好地理解相关内容。

#### 词向量化基础

计算机在处理海量的文本信息时，一个重要的处理方法就是将文本信息向量化表示，即将每个文本中包含的词语进行向量化存储。
MLlib中为了能够处理海量的文本，采用的是一种低维向量的方法来表示词组。这样做的最大的好处是，对于选定的词组在向量空间中能够更加紧密地靠近，从而对文本特征提取和转换提供好处。这里文本距离的计算请参考Kmeans一章中文本距离的计算方法。
目前MLlib中词向量转换采用的是skip-gram模型来实现，这个也是神经网络学习方法的一个特定学习方式，具体如图12-1所示。

![image-20201012002250744](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201012002250744.png)



上图中w(t)是输入的文本，PROJECTION对应的是模型参数，而输出的是每个单词出现的概率，因此整体skip-gram可以用如下公式表示：

![image-20201012002331505](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201012002331505.png)

其中ω代表整体文章，p(c|ω,θ)是指在模型参数θ的情况下，某个语句c在ω中出现的概率，因此整体就转化成寻找一个特定θ从而使得f(x)最大化。



### 基于卡方检验的特征选择

> 卡方检验是用途非常广泛的一种假设检验方法，它在分类资料统计推断中一般用于检验一个样本是否符合预期的一个分布。其计算==原理==就是，把待测定的数据分布分成几个互不相交的区域，每个区域的理论概率可知，之后查看测定结果值落在这些区域的频率，是否跟理论概率差不多。
>
> 一般来说卡方检验就是统计样本的实际观测值与理论推断值之间的偏离程度，实际观测值与理论推断值之间的偏离程度就决定了卡方值的大小。==卡方值越大，越不符合，偏差越小，卡方值就越小，越趋于符合，若量值完全相等时，卡方值就为0，表明理论值完全符合==。
>
> MLlib中，卡方检验主要是用于对结果进行检验，考核通过程序算法做出的特征提取是否符合预期。



总结：

卡方检验是用于检验实际值与理论值偏差的统计量。一般来说可以先假定两个量相互独立之后对它进行计算，推翻或验证原先的假定量。如果偏差小于阈值，则认定假设真实可信，而当大于阈值的话就认为偏差过大，原先的假设不成立。



## chapter13 实战-鸢尾花分析

本章主要研究一个较为基础的、==经典的数据挖掘任务==，包括数据的`预处理`、`数据的分析性挖掘`和多`种MLlib算法`的使用。这里笔者选择了一个经典的数据集，即鸢尾花数据集。

实战中我们将带领读者去研究不同的鸢尾花的生长分布，以及种类的判定方法，其中会使用到回归分析方法以及决策树方法，这些都是现实中常用的数据挖掘方法。在回归分析方法中，我们将比较线性回归和逻辑回归在分析相同数据集上的异同

鸢尾花数据集是由杰出的统计学家R.A.Fisher在20世纪30年代中期创建的，`它是公认的、用于数据挖掘的最著名的数据集。`





## 推荐系统

推荐系统本质上就是一个信息过滤系统，通常分为：召回、排序、重排序这3个环节，每个环节逐层过滤，最终从海量的物料库中筛选出几十个用户可能感兴趣的物品推荐给用户。

![image-20201016204944007](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201016204944007.png)



推荐系统的应用场景通常分为以下两类：

- **基于用户维度的推荐：**根据用户的历史行为和兴趣进行推荐，比如淘宝首页的猜你喜欢、抖音的首页推荐等。
- **基于物品维度的推荐：**根据用户当前浏览的标的物进行推荐，比如打开京东APP的商品详情页，会推荐和主商品相关的商品给你。



###  搜索、推荐、广告三者的异同

搜索和推荐是AI算法最常见的两个应用场景，在技术上有相通的地方。这里提到广告，主要考虑很多没做过广告业务的同学不清楚为什么广告和搜索、推荐会有关系，所以做下解释。

- **搜索：**有明确的搜索意图，搜索出来的结果和用户的搜索词相关。
- **推荐：**不具有目的性，依赖用户的历史行为和画像数据进行个性化推荐。
- **广告：**借助搜索和推荐技术实现广告的精准投放，可以将广告理解成搜索推荐的一种应用场景，技术方案更复杂，涉及到智能预算控制、广告竞价等。



### 推荐系统的整体架构

<img src="https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201016205201854.png" alt="image-20201016205201854" style="zoom:80%;" />

上面是推荐系统的整体架构图，自下而上分成了多层，各层的主要作用如下：

- **数据源：**推荐算法所依赖的各种数据源，包括物品数据、用户数据、行为日志、其他可利用的业务数据、甚至公司外部的数据。
- **计算平台：**负责对底层的各种异构数据进行清洗、加工，离线计算和实时计算。
- **数据存储层：**存储计算平台处理后的数据，根据需要可落地到不同的存储系统中，比如Redis中可以存储用户特征和用户画像数据，ES中可以用来索引物品数据，Faiss中可以存储用户或者物品的embedding向量等。
- **召回层：**包括各种推荐策略或者算法，比如经典的协同过滤，基于内容的召回，基于向量的召回，用于托底的热门推荐等。为了应对线上高并发的流量，召回结果通常会预计算好，建立好倒排索引后存入缓存中。
- **融合过滤层：**触发多路召回，由于召回层的每个召回源都会返回一个候选集，因此这一层需要进行融合和过滤。
- **排序层：**利用机器学习或者深度学习模型，以及更丰富的特征进行重排序，筛选出更小、更精准的推荐集合返回给上层业务。

从数据存储层到召回层、再到融合过滤层和排序层，候选集逐层减少，但是精准性要求越来越高，因此也带来了计算复杂度的逐层增加，这个便是推荐系统的最大挑战。

其实对于推荐引擎来说，最核心的部分主要是两块：特征和算法。

<img src="https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201016205304260.png" alt="image-20201016205304260" style="zoom:80%;" />



特征计算由于数据量大，通常采用大数据的离线和实时处理技术，像Spark、Flink等，然后将计算结果保存在Redis或者其他存储系统中（比如HBase、MongoDB或者ES），供召回和排序模块使用。

召回算法的作用是：从海量数据中快速获取一批候选数据，要求是快和尽可能的准。这一层通常有丰富的策略和算法，用来确保多样性，为了更好的推荐效果，某些算法也会做成近实时的。

排序算法的作用是：对多路召回的候选集进行精细化排序。它会利用物品、用户以及它们之间的交叉特征，然后通过复杂的机器学习或者深度学习模型进行打分排序，这一层的特点是计算复杂但是结果更精准。



### 推荐系统常用算法

#### 协同过滤

chapter04

找相似，核心就是余弦相似度，



































