# Spark MLlib学习笔记

Spark的MLlib库基本上已经过时，现在主流的是ml库，所以玩玩就好。

- spark官方推荐使用ml, 因为ml功能更全面更灵活，未来会主要支持ml，mllib很有可能会被废弃(据说可能是在spark3.0中deprecated）。
- ml中无论是什么模型，都提供了统一的算法操作接口，比如模型训练都是`fit`；不像mllib中不同模型会有各种各样的`trainXXX`。
- mllib在spark2.0之后进入`维护状态`, 这个状态通常只修复BUG不增加新功能。

## reduceByKey和groupByKey区别

先上结论

> 在对大数据进行复杂计算时，reduceByKey优于groupByKey。
>
> 另外，如果仅仅是group处理，那么以下函数应该优先于 groupByKey ：
> 　　（1）、combineByKey 组合数据，但是组合之后的数据类型与输入时值的类型不一样。
> 　　（2）、foldByKey合并每一个 key 的所有值，在级联函数和“零值”中使用。





reduceByKey用于对每个key对应的多个value进行merge操作，最重要的是它能够在本地先进行merge操作，并且merge操作可以通过函数自定义。

groupByKey也是对每个key进行操作，但只生成一个sequence。需要特别注意“Note”中的话，它告诉我们：如果需要对sequence进行aggregation操作（注意，groupByKey本身不能自定义操作函数），那么，选择reduceByKey/aggregateByKey更好。这是因为groupByKey不能自定义函数，我们需要先用groupByKey生成RDD，然后才能对此RDD通过map进行自定义函数操作。



用法（结果一样，内部处理方式不同）

~~~scala
val words = Array("one", "two", "two", "three", "three", "three")  
  
val wordPairsRDD = sc.parallelize(words).map(word => (word, 1))  

val wordCountsWithReduce = wordPairsRDD.reduceByKey(_ + _)  
val wordCountsWithGroup = wordPairsRDD.groupByKey().map(t => (t._1, t._2.sum))  
~~~



当采用reduceByKeyt时，Spark可以在每个分区移动数据之前将待输出数据与一个共用的key结合。借助下图可以理解在reduceByKey里究竟发生了什么。 注意在数据对被搬移前同一机器上同样的key是怎样被组合的(reduceByKey中的lamdba函数)。然后lamdba函数在每个区上被再次调用来将所有值reduce成一个最终结果。整个过程如下：



![image-20201007171314279](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201007171314279.png)



当采用groupByKey时，由于它不接收函数，spark只能先将所有的键值对(key-value pair)都移动，这样的后果是集群节点之间的开销很大，导致传输延时。整个过程如下：

![image-20201007171355789](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201007171355789.png)







## RDD

RDD可以看成是一个简单的“数组”，对其进行操作只需要调用有限的数组中的方法即可。与一般数组的区别在于：RDD是分布式存储，可以更好地利用现有的云数据平台，并在内存中运行（省去大量与硬盘的IO）。本质是存储在不同节点计算机中的数据集



在Spark中，弹性指的是数据的存储方式，即数据在节点中进行存储的时候，既可以使用内存也可以使用硬盘；弹性还有一层意思是，RDD具有很强的容错性，具体指Spark在运行计算的过程中，不会因为某个节点错误而使得整个任务失败。不同节点并发运行的数据，如果在某一个节点发生错误时，RDD会自动将其在不同的节点中重试。



### RDD工作原理

RDD可以将其看成一个分布在不同节点中的分布式数据集，并将数据以数据块（Block）的形式存储在各个节点的计算机中，如下图所示



![image-20201007175229211](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201007175229211.png)



从上图可以看到，每个Block Master 管理着若干个BlockSlave，而每个BlockSlaver又管理着若干个BlockNode。当BlockSlave获得了每个Node节点的地址，又会反向BlockMaster注册每个Node的基本信息，形成分层管理。

而对于某个节点中存储的数据，如果使用频率较多，则BlockMaster会将其缓存在自己的内存中，这样如果以后需要调用这些数据，则可以直接从BlockMaster中读取。对于不再使用的数据，BlockMaster会向BlockSlave发送一组命令予以销毁。



> tip
>
> 窄依赖便于在单一节点上按次序执行任务，使任务可控。宽依赖更多的是考虑任务的交互和容错性。





## chapter04 MLlib 基本概念

RDD将存储数据转化成向量和矩阵的形式进行存储和计算，这样将数据定量化表示，能更准确地整理和分析结果



### MLlib 基本数据类型

MLlib支持较多的数据格式，从最基本的Spark数据集RDD到部署在集群中的向量和矩阵

MLlib目前只支持整数与浮点数。其他类型均不支持，这与MLlib用途有关，主要用于数值计算



MLlib基本数据类型

| 类型名称           | 释义                                                 |
| ------------------ | ---------------------------------------------------- |
| Local vector       | 本地向量集。主要向spark提供一组可进行操作的数据集和  |
| Labeled point      | 向量标签。让用户能够分类不同的数据集和               |
| Local matrix       | 本地矩阵。将数据集合以矩阵形式存储在本地计算机中     |
| Distributed matrix | 分布式矩阵。将数据集合以矩阵形式存储在分布式计算机中 |



### 两组数据相关系数计算

相关系数是一种用来反映变量之间相关关系密切程度的统计指标，在现实中一般用于对两组数据的`拟合`和`相似程度`进行定量化分析。常用的一般是皮尔逊相关系数，MLlib中默认的相关系数求法也是使用皮尔逊相关系数发。斯皮尔曼相关系数用的比较少，但是其能够比较好地反映不同数据集的趋势程度。



皮尔逊相关系数计算公式
$$
\rho_{xy} = 
\frac{
	\sum(x-\overline{x})(y-\overline{y})
}{
	\sqrt{\sum(x-\overline{x}){2}  \sum(y-\overline{y}){2}}
}
$$
可以看做是两组数据的向量夹角的余弦，用来描述两组数据的分开程度。



斯皮尔曼相关系数计算公式
$$
\rho_{xy} = 1-
\frac{
	6\sum(x_i-y_i){2}
}{
	n(n^{2}-1)
}
$$

> n为数据个数



总结：

皮尔逊相关系数代表两组数据的余弦分开程度，表示随着数据量的增加，两组数据差别将增大

斯皮尔曼相关系数更注重两组数据的拟合程度，即两组数据随着数据量的增加而增长曲线不变



## chapter05 协同过滤算法



协同过滤算法是最常用的推荐算法，主要有两种具体形式：基于用户的推荐算法 和 基于物品的推荐算法。  推荐算法的基础是基于两个对象之间的相关性。



### 协同过滤

协同过滤算法是一种基于群体用户或者物品的典型推荐算法，是目前常用推荐算法中最常用和最经典的算法。

协同过滤算法主要有两种

- 一是通过考察具有相同爱好的用户对相同物品的评分标准进行计算
- 而是考察具有相同特质的物品从而推荐给选择了某件物品的用户

总体来说，协同过滤算法是建立在基于某种物品和用户之间相互关联的数据关系之上。



#### 基于用户的推荐

一句话概括：给推荐`志趣相投`的，俩人有两个物品都喜欢，则第一个人喜欢的另外一个物品，第二个人可能也喜欢

#### 基于物品的推荐

给推荐`物以类聚`的，以已有物品为线索去进行相似度计算从而推荐给特定的目标用户。



#### 协同过滤算法的不足

> 基于用户的推荐算法，针对某些热点物品的处理不够准确，对于一些常用的物品推荐，其计算结果往往排在推荐的首位，而这样的推荐没有实际应用意义。同时基于用户的推荐算法，往往数据量较为庞大，计算费事，由于热点的存在准确度也很成问题
>
> 基于物品的推荐算法相对于基于用户的推荐算法，其数据量小很多，可以较为容易地生成推荐值，但是其存在推荐同样（同类型）物品的问题。例如，用户购买了某件商品，那么推荐系统可能会继续推荐相同类型的商品给用户，用户在购买一件商品后，绝对不会再购买同类型的商品，这样的推荐是失败的。

### 相似度度量

协同过滤的最重要部分是相似度求得



#### 欧几里得距离

三维空间两个点的真实距离。

由于在欧几里得距离相似度计算中，最终数值的大小与相似度成反比，因此在实际应用场景中常常使用欧几里得距离的倒数作为相似度值，即 d/d+1 作为近似值

> 就是计算勾股定理的长边

#### 余弦相似度

与欧几里得距离相似，余弦相似度也将特定目标，即物品或者用户作为坐标上的点，但不是坐标原点。基于此与特定的被计算目标进行夹角计算。

> 就是计算夹角



#### 欧几里得相似度和余弦相似度比较

欧几里得相似度是以目标绝对距离为衡量的标准，而余弦相似度是以目标差异的大小作为衡量标准。

![image-20201008161851096](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201008161851096.png)

从上图可以看出，欧几里得相似度注重目标之间的差异，与目标在空间中的位置直接相关。而余弦相似度是不同目标在空间中的夹角，更加表现在前进趋势上的差异。

一般来说欧几里得用来表现不同目标的绝对差异性，分析目标之间的相似度与差异情况。余弦相似度更多的是对目标从方向趋势上区分，对特定坐标数字不敏感。



### ALS算法 - 交替最小二乘法

交替最小二乘法是统计分析中最常用的逼近计算中的一种算法，其交替计算结果使得最终结果尽可能地逼近真实结果。

#### 最小二乘法（LS算法）

LS算法是ALS算法的基础，LS算法是一种数学优化技术。通过最小化误差的平方和寻找数据的最佳函数配比。利用最小二乘法可以简便地求得未知数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。最小二乘法还可用于曲线拟合。其他一些优化问题也可通过最小化能量或最大化熵用最小二乘法来表达。

LS算法原理

![image-20201008164026358](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201008164026358.png)

若干个点分布在向量空间中，如果希望找出一条直线和这些点达到最佳匹配。最简单的一个方法就是希望这些点到直线的距离值最小。

f(x) 是直接拟合公式，即目标函数
$$
f(x)=ax+b
$$

$$
\delta=\sum(f(x_i)-y_i)^2
$$





#### 交替最小二乘法（ALS算法）

比较难，详细理解还需要再百度

假设基于用户名、物品表的用户评分矩阵可以被分解成2个较为小型化的矩阵，即矩阵 U 和矩阵 V 。因此可以将原始矩阵近似表示为：W=U*V

这里U和V分别表示用户和物品的矩阵，在MLlib的ALS算法中，首先对U或者V矩阵随机化生成，之后固定某一个特定对象，去求另一个未随机化的矩阵对象。然后利用被求取的矩阵对象去求随机化矩阵对象。最后两个对象相互迭代计算，求取与实际矩阵差异达到程序设定的最下阈值位置。如下所示

![image-20201008171214175](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201008171214175.png)



## chapter06 线性回归

回归分析是一种用来确定两种或两种以上变量间相互依赖的定量关系的统计分析方法。分析可以按以下要素分类

- 按照涉及的自变量的多少，分为回归和多重回归分析
- 按照自变量的多少，可分为一元回归分析和多元回归分析
- 按照自变量与因变量之间的关系类型，可分为线性回归分析和非线性回归分析

> 在回归分析中，只包括一个自变量和一个因变量，且二者关系可用一条直线近似表示，称为一元线性回归分析。
>
> 如果回归分析中包括两个或两个以上的自变量，且因变量与自变量之间是线性关系，则称为多重线性回归分析。



### 随机梯度下降算法

机器学习中回归算法有 神经网络回归算法、蚁群回归算法、支持向量机回归算法等。MLlib中使用的是较为经典的随机梯度下降算法，它充分利用Spark框架的迭代计算特性，通过不停地判断和选择当前目标下的最优路径，从而能够在最短路径下达到最优的结果，继而提过大数据计算效率



道士下山

![image-20201008173958281](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201008173958281.png)

> 简单来说，梯度下降就是从山顶找一条最短的路走到山脚最低的地方。但是因为选择方向的原因，我们找到的的最低点可能不是真正的最低点。如图所示，黑线标注的路线所指的方向并不是真正的地方。
>
> 总结起来就一句话：随机选择一个方向，然后每次迈步都选择最陡的方向，直到这个方向上能达到的最低点。



梯度下降算法模型

![image-20201008174753527](https://raw.githubusercontent.com/j8130/picBed/master/img/image-20201008174753527.png)



### 回归的过拟合

详细理解需要百度

消除回归算法中的误差，通过增加系数等方法修正过拟合。

消除过拟合的过程称为回归的正则化。正则化使用较多的一般有两种方法，lasso回归（L1回归）和岭回归（L2回归），其目的是通过对最小二乘法估计加入惩罚因素，使某些系数的估计为0。



### 线性回归

MLlib中，线性回归的基本数据是严格按照数据格式进行设置。例如，如果想求得公式
$$
y=2x_1+3x_2
$$
的系数，那么需要在数据基础中设置2个x值，并且在其前部设置y值

~~~markdown
1,0 1
2,0 2
3,0 3
5,1 4
7,6 1
9,4 5
6,3 3

逗号前面的数值是根据不同的数据求出的结果值，而每个系数x值依次地被排列在其后，这些就是数据收集的规则：Y=a+bX

~~~





































































































